{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7781456,"sourceType":"datasetVersion","datasetId":4553642},{"sourceId":7864053,"sourceType":"datasetVersion","datasetId":4613420}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport numpy as np\n\n# Assuming the training data is named 'train.csv' and the testing data is named 'test.csv'\ntrain_data_path = '/kaggle/input/new-dataset/train.csv'\ntest_data_path = '/kaggle/input/new-dataset/test.csv'\n\n# Load the data\ntrain_data = pd.read_csv(train_data_path, sep=' ', header=None)\ntest_data = pd.read_csv(test_data_path, sep=' ', header=None)\n\n# Split train data into features and labels\nX = train_data.iloc[:, 1:].values\ny = train_data.iloc[:, 0].values - 1  # Assuming labels are 1-12, we shift to 0-11 for PyTorch\n\n# Normalize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define a PyTorch dataset\nclass CustomDataset(Dataset):\n    def __init__(self, features, labels=None, mode='train'):\n        self.features = features\n        self.labels = labels\n        self.mode = mode\n    \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, idx):\n        if self.mode == 'train' or self.mode == 'val':\n            return torch.tensor(self.features[idx], dtype=torch.float), torch.tensor(self.labels[idx], dtype=torch.long)\n        else:\n            return torch.tensor(self.features[idx], dtype=torch.float)\n\n# Create datasets and dataloaders\ntrain_dataset = CustomDataset(X_train, y_train, mode='train')\nval_dataset = CustomDataset(X_val, y_val, mode='val')\ntest_dataset = CustomDataset(scaler.transform(test_data.values), mode='test')\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-25T13:31:33.792826Z","iopub.execute_input":"2024-03-25T13:31:33.793351Z","iopub.status.idle":"2024-03-25T13:31:38.717302Z","shell.execute_reply.started":"2024-03-25T13:31:33.793315Z","shell.execute_reply":"2024-03-25T13:31:38.715664Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class MultiLayerNNModified(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(MultiLayerNNModified, self).__init__()\n        self.fc1 = nn.Linear(input_size, 512)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.dropout1 = nn.Dropout(0.1)\n        self.fc2 = nn.Linear(512, 256)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.dropout2 = nn.Dropout(0.1)\n        self.fc3 = nn.Linear(256, 128)\n        self.bn3 = nn.BatchNorm1d(128)\n        self.dropout3 = nn.Dropout(0.1)\n        self.fc4 = nn.Linear(128, output_size)\n\n    def forward(self, x):\n        x = F.leaky_relu(self.bn1(self.fc1(x)), negative_slope=0.01)\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.bn2(self.fc2(x)), negative_slope=0.01)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.bn3(self.fc3(x)), negative_slope=0.01)\n        x = self.dropout3(x)\n        x = self.fc4(x)\n        return x\n\n\n# Initialize the model, loss criterion, and optimizer\nmodel = MultiLayerNNModified(input_size=400, output_size=12)  # 400 features and 12 possible labels\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, min_lr=1e-6, verbose=True)\n\n# Function to train the model\ndef train_model(model, train_loader, val_loader, criterion, optimizer, epochs=100):\n    for epoch in range(epochs):\n        model.train()  # Set model to training mode\n        train_loss = 0\n        for inputs, labels in train_loader:\n            optimizer.zero_grad()          # Reset gradients\n            outputs = model(inputs)        # Forward pass\n            loss = criterion(outputs, labels)\n            loss.backward()                # Backward pass\n            optimizer.step()               # Update weights\n            train_loss += loss.item() * inputs.size(0)\n        train_loss = train_loss / len(train_loader.dataset)\n\n        # Validation phase\n        model.eval()   # Set model to evaluation mode\n        val_loss = 0\n        correct = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item() * inputs.size(0)\n                _, preds = torch.max(outputs, 1)\n                correct += torch.sum(preds == labels.data)\n        val_loss = val_loss / len(val_loader.dataset)\n        val_acc = correct.double() / len(val_loader.dataset)\n\n        print(f'Epoch {epoch+1}/{epochs}.. '\n              f'Train loss: {train_loss:.4f}.. '\n              f'Validation loss: {val_loss:.4f}.. '\n              f'Validation accuracy: {val_acc:.4f}')\n        \n        scheduler.step(val_loss)        \n\n# Train the model\ntrain_model(model, train_loader, val_loader, criterion, optimizer, epochs=100)","metadata":{"execution":{"iopub.status.busy":"2024-03-25T13:31:38.719481Z","iopub.execute_input":"2024-03-25T13:31:38.719844Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/100.. Train loss: 1.6645.. Validation loss: 1.3921.. Validation accuracy: 0.3822\nEpoch 2/100.. Train loss: 1.3884.. Validation loss: 1.1368.. Validation accuracy: 0.5069\nEpoch 3/100.. Train loss: 1.2163.. Validation loss: 1.0117.. Validation accuracy: 0.5790\nEpoch 4/100.. Train loss: 1.1107.. Validation loss: 0.9294.. Validation accuracy: 0.6315\nEpoch 5/100.. Train loss: 1.0293.. Validation loss: 0.8257.. Validation accuracy: 0.6321\nEpoch 6/100.. Train loss: 0.9627.. Validation loss: 0.7725.. Validation accuracy: 0.6718\nEpoch 7/100.. Train loss: 0.9123.. Validation loss: 0.7047.. Validation accuracy: 0.6947\nEpoch 8/100.. Train loss: 0.8708.. Validation loss: 0.6802.. Validation accuracy: 0.7125\nEpoch 9/100.. Train loss: 0.8403.. Validation loss: 0.6274.. Validation accuracy: 0.7340\nEpoch 10/100.. Train loss: 0.8080.. Validation loss: 0.6017.. Validation accuracy: 0.7537\nEpoch 11/100.. Train loss: 0.7685.. Validation loss: 0.5633.. Validation accuracy: 0.7594\nEpoch 12/100.. Train loss: 0.7521.. Validation loss: 0.5557.. Validation accuracy: 0.7640\nEpoch 13/100.. Train loss: 0.7248.. Validation loss: 0.5578.. Validation accuracy: 0.7710\nEpoch 14/100.. Train loss: 0.7000.. Validation loss: 0.5264.. Validation accuracy: 0.7827\nEpoch 15/100.. Train loss: 0.6817.. Validation loss: 0.4943.. Validation accuracy: 0.8027\nEpoch 16/100.. Train loss: 0.6640.. Validation loss: 0.4958.. Validation accuracy: 0.7954\nEpoch 17/100.. Train loss: 0.6355.. Validation loss: 0.4648.. Validation accuracy: 0.8074\nEpoch 18/100.. Train loss: 0.6156.. Validation loss: 0.4543.. Validation accuracy: 0.8192\nEpoch 19/100.. Train loss: 0.5992.. Validation loss: 0.4426.. Validation accuracy: 0.8232\nEpoch 20/100.. Train loss: 0.5891.. Validation loss: 0.4308.. Validation accuracy: 0.8255\nEpoch 21/100.. Train loss: 0.5608.. Validation loss: 0.4077.. Validation accuracy: 0.8359\nEpoch 22/100.. Train loss: 0.5545.. Validation loss: 0.3944.. Validation accuracy: 0.8463\nEpoch 23/100.. Train loss: 0.5383.. Validation loss: 0.3942.. Validation accuracy: 0.8486\nEpoch 24/100.. Train loss: 0.5284.. Validation loss: 0.3909.. Validation accuracy: 0.8458\nEpoch 25/100.. Train loss: 0.5110.. Validation loss: 0.3799.. Validation accuracy: 0.8542\nEpoch 26/100.. Train loss: 0.4930.. Validation loss: 0.3805.. Validation accuracy: 0.8501\nEpoch 27/100.. Train loss: 0.4824.. Validation loss: 0.3666.. Validation accuracy: 0.8597\nEpoch 28/100.. Train loss: 0.4730.. Validation loss: 0.3589.. Validation accuracy: 0.8605\nEpoch 29/100.. Train loss: 0.4650.. Validation loss: 0.3519.. Validation accuracy: 0.8710\nEpoch 30/100.. Train loss: 0.4525.. Validation loss: 0.3481.. Validation accuracy: 0.8652\nEpoch 31/100.. Train loss: 0.4406.. Validation loss: 0.3406.. Validation accuracy: 0.8710\n","output_type":"stream"}]},{"cell_type":"code","source":"def generate_predictions(model, test_loader):\n    model.eval()  # Set model to evaluation mode\n    predictions = []\n    with torch.no_grad():\n        for inputs in test_loader:\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            predictions.extend(preds.cpu().numpy() + 1)  # Adjust predictions to match original label range (1-12)\n    return predictions\n\n# Generate predictions\npredictions = generate_predictions(model, test_loader)\n\n# Prepare the submission file\nsubmission = pd.DataFrame({'ID': np.arange(0, len(predictions)), 'Target': predictions})\nsubmission.to_csv('/kaggle/working/submission3.csv', index=False)\n\nprint(\"Submission file has been created.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}